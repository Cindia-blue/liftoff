å¥½çš„ï¼Œä»¥ä¸‹æ˜¯æ•´ç†å¥½çš„ RCA æ–‡æ¡£ï¼Œç”¨äºè®°å½•å¹¶æ€»ç»“ â€œContainerd shim crash å¯¼è‡´å®¹å™¨çŠ¶æ€ Unknownâ€ çš„æ•…éšœåˆ†æä¸åº”å¯¹æµç¨‹ã€‚æ ¼å¼ä¸ºæ­£å¼ç»“æ„åŒ–æ–‡æ¡£ï¼Œå¯ç›´æ¥ç”¨äº incident å¤ç›˜ã€æ•…éšœçŸ¥è¯†åº“æˆ– onboarding è®­ç»ƒèµ„æ–™ã€‚

â¸»

RCA Report: ContainerD Shim Crash â€“ Container in Unknown State

â¸»

Incident Summary

Symptom:
Containers unexpectedly entered Unknown status during runtime. Upon investigation, logs showed that the associated containerd-shim process exited with code 137.

Impact:
	â€¢	Container becomes non-functional and unresponsive.
	â€¢	crictl ps -a or ctr tasks list shows container in UNKNOWN or missing from active task list.
	â€¢	Resources such as mount points and snapshots are not released, leading to potential leakage.

â¸»

Root Cause Hypothesis

Exit code 137 indicates the shim process was killed by signal SIGKILL. This usually implies the system or user forcibly terminated the process.

Possible Causes:
	1.	Out of Memory (OOM)
	â€¢	Either the container or shim process was killed by the kernelâ€™s OOM killer.
	â€¢	Shim processes are not typically memory-heavy, but under resource starvation, they can still be evicted.
	2.	Cgroup Resource Limits
	â€¢	The container or shim exceeded memory or CPU usage limits imposed by Kubernetes or systemd slices.
	3.	Host Resource Exhaustion
	â€¢	System-wide memory, PID, or file descriptor exhaustion may cause random process eviction or kernel panic behavior.
	4.	Human or External Automation Kill
	â€¢	Shim process may have been manually killed, or terminated by external cleanup scripts or sandboxed environments.

â¸»

Verification Steps
	1.	Check Metadata Integrity
	â€¢	crictl inspect <container> or ctr containers info <id> to verify metadata still exists.
	â€¢	ctr snapshots ls to confirm the underlying snapshot is present.
	â€¢	Check /run/containerd/io.containerd.runtime.v2.task/... to confirm if mount points are still active.
	2.	Investigate Kill Reason
	â€¢	journalctl -u containerd or full journalctl to trace containerd-shim crash stack.
	â€¢	dmesg | grep -i kill or grep -i oom to verify kernel OOM involvement.
	â€¢	Inspect cgroup metrics for limit breaches (cat /sys/fs/cgroup/...).

â¸»

Remediation / Recovery Strategy

If metadata is intact:
	â€¢	Option 1: Recreate task directly using ctr tasks start if container state is known.
	â€¢	Option 2: Force delete task using ctr tasks delete --force, then re-create container and restore from existing image/snapshot.

If metadata is corrupted or missing:
	â€¢	Clean up stale mount points manually from /run/containerd/....
	â€¢	Ensure snapshot layers are released using ctr snapshots rm.
	â€¢	Rebuild container from image to regenerate metadata tree.

â¸»

Long-Term Mitigation Suggestions
	â€¢	Add monitoring for shim process lifecycle (watchdog, liveness metrics).
	â€¢	Tune cgroup limits and host resource thresholds for shim-resident workloads.
	â€¢	Add logging/tracing hooks around shim exit events in containerd.
	â€¢	Automate safe cleanup of zombie mount points during node reconciliation.

â¸»

å¦‚æœä½ æƒ³åŠ ä¸€ä¸ª ğŸ§­ ç›¸å…³è·¯å¾„å›¾ / ä»£ç  anchor çš„è¡¥å……éƒ¨åˆ†ï¼Œæˆ‘å¯ä»¥ç»§ç»­æ ‡æ³¨ç›¸å…³ä»£ç ä½ç½®ä¸è°ƒç”¨é“¾ã€‚æ˜¯å¦ä¹Ÿå¸Œæœ›æˆ‘ç”Ÿæˆä¸€ç‰ˆç®€åŒ– Markdown æ ¼å¼ï¼ˆå¦‚ä½ è¦å­˜è¿› Notion / Confluence / Obsidianï¼‰ï¼Ÿä½ è¯´ä¸€å£°å°±å¥½ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ç»§ç»­è¿›å…¥æ¨¡å—äºŒçš„ Code Reviewã€‚



éå¸¸æ£’ï¼ä½ çš„å›ç­”å·²ç»å…·å¤‡å®Œæ•´çš„å®¡æŸ¥æ„è¯†ï¼šæ­£ç¡®è¯†åˆ«äº†é‡è¯•æœºåˆ¶çš„ç›²ç‚¹ï¼ˆæ—  backoffã€æ— é”™è¯¯åˆ†ç±»ï¼‰ã€å¹¶å‘é—®é¢˜ï¼Œä»¥åŠ observability ç¼ºå¤±ï¼Œå¹¶æ¸…æ™°åœ°æå‡ºæ”¹è¿›å»ºè®®ã€‚æ¥ä¸‹æ¥æˆ‘ä¼šé€æ¡ç»™ä½ æ¨¡æ‹Ÿ Code Review é£æ ¼çš„é«˜è´¨é‡åé¦ˆï¼Œè®©ä½ ä¹ æƒ¯å¦‚ä½•åœ¨å›¢é˜Ÿåä½œä¸­è¡¨è¾¾ä¸å›åº”æŠ€æœ¯æ”¹è¿›ã€‚

â¸»

Q1: Retry logic review & improvements

ä½ è¯´å¾—å¾ˆå¥½ï¼šè¯¥é‡è¯•æœºåˆ¶æ˜¯ç›²é‡è¯•ï¼ˆblind retryï¼‰ï¼Œæ²¡æœ‰ backoff ç­–ç•¥ï¼Œä¹Ÿæ²¡æœ‰åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•ã€‚

è¡¥å……å»ºè®®ï¼š

// Bad: time.Sleep(i * time.Second) is not true exponential backoff.
// Suggest: use backoff libs (e.g. exponential, capped jittered).

Improvement: Introduce a proper backoff strategy, e.g., exponential + jitter.

backoff := time.Duration(1<<i) * time.Second + jitter()
time.Sleep(backoff)



Further: Introduce error classification logic. Only retry for network / timeout / transient errors:

if isRetriable(err) { ... } else { return err }



â¸»

Q2: Behavior under high-frequency upload

ä½ æŒ‡å‡º concurrent execution ä¼šå¯¼è‡´å†²çªæˆ–é‡å¤ä¸Šä¼ ï¼Œè¿™æ˜¯éå¸¸é‡è¦çš„è§‚å¯Ÿç‚¹ã€‚

Enhancement Suggestion:
	â€¢	If multiple uploaders share the same blob, they may trigger redundant transfers.
	â€¢	Introduce deduplication using blob digest as lock key.
	â€¢	Suggest wrapping the transfer call in a sync.Once / singleflight-style structure:

lock := blobLockPool.Get(digest)
lock.Lock()
defer lock.Unlock()



Danger: If each retry also causes a high IOPS write to S3 / registry, the system may degrade quickly under load.

â¸»

Q3: Missing metrics / logs

ä½ æåˆ°â€œfailure reason / retry count / blob digestâ€éƒ½åº”è®°å½•ï¼Œå®Œå…¨æ­£ç¡®ã€‚

Metrics Suggestions:
	â€¢	transfer_retries_total{status="success"|"failed"} â€” counter
	â€¢	transfer_duration_seconds â€” histogram
	â€¢	transfer_failed_reasons{type="timeout|403|disk"} â€” label-based classification

Log Suggestions:
	â€¢	Add log.WithFields() logging with:
	â€¢	blob digest
	â€¢	retry index
	â€¢	error type & code
	â€¢	time used per attempt

â¸»

âœ… æ€»ç»“ï¼šä½ å·²ç»å±•ç¤ºå‡º Staff/Senior Review çº§åˆ«çš„æ„è¯†

ç»´åº¦	ä½ çš„è¡¨ç°	å»ºè®®å¼ºåŒ–
Retryç»“æ„æ€§	è¯†åˆ« retry å¤ªç²—æš´ã€éœ€ backoff	å¼•å…¥ jitter/backoff å‚è€ƒåº“
å¹¶å‘å®‰å…¨æ€§	æŒ‡å‡ºå¹¶å‘é£é™©ï¼Œå»ºè®®é”	å¯æ¨èå…·ä½“å®ç°ï¼šsync.Map + singleflight
å¯è§‚æµ‹æ€§	metrics/log éƒ½è¦†ç›–	å¯é™„å®é™… Prometheus/trace æ ·ä¾‹


â¸»

ä½ å·²ç» ready è¿›å…¥æ¨¡å—ä¸‰ verbal expression è®­ç»ƒäº†ï¼Œè¦ä¸è¦æˆ‘ä»¬æ¥ç»ƒä¸€ä¸‹ snapshotter çš„è§’è‰²æè¿°ï¼Ÿä½ è¯´ä¸€å£°å°±è¿›ã€‚




ä½ æ•´ä½“çš„è¡¨è¾¾æ˜¯å‡†ç¡®ã€æœ‰æ¡ç†çš„ï¼Œå±•ç°äº†å¯¹ snapshotter æ¨¡å—çš„ç†è§£â€”â€”å°¤å…¶æ˜¯æŠ½è±¡å±‚ã€ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€åº•å±‚è§£è€¦çš„æè¿°éå¸¸åˆ°ä½ã€‚æ¥ä¸‹æ¥æˆ‘ä¼šå¸®ä½ æ¶¦è‰²å¹¶ç»“æ„åŒ–è¿™æ®µå›ç­”ï¼Œä½¿å®ƒæ›´ç¬¦åˆ é«˜çº§è‹±æ–‡è¡¨è¾¾åœºæ™¯ï¼ˆæŠ€æœ¯æ±‡æŠ¥ / onboarding / system overview meetingï¼‰ çš„è¯­æ°”å’ŒèŠ‚å¥ã€‚

â¸»

ä¼˜åŒ–åçš„å›ç­”ç‰ˆæœ¬ï¼ˆå¯ä»¥ç”¨äºæœ—è¯»/èƒŒè¯µï¼‰

In containerd, the snapshotter is a core module responsible for managing the lifecycle of container filesystem snapshots. This includes preparing, mounting, committing, and removing snapshots.

Functionally, snapshotter serves as an abstraction layer between containerd and the underlying storage backend. It allows containerd to define a unified interface for snapshot operations, while enabling the actual implementation to vary â€” such as using overlayfs, btrfs, or fuse-based drivers â€” without changing the containerd core logic.

For example, when using overlayfs as the backend, the snapshotter will orchestrate the creation of upper and lower directories, configure the work directory, and prepare a layered mount. All this is exposed to containerd as a clean interface, so the higher-level components do not need to understand or manage mount intricacies.

This design brings modularity, pluggability, and stability. It helps containerd scale across environments with different storage requirements while maintaining a consistent runtime API.

â¸»

ç‚¹è¯„ä½ çš„è¡¨è¾¾ä¼˜ç‚¹ï¼š
	â€¢	å¼ºè°ƒäº† â€œisolation from backendâ€ å’Œ â€œarchitecture robustnessâ€ï¼Œéå¸¸æ£’ï¼›
	â€¢	ä½¿ç”¨äº† â€œlayered mountâ€ï¼Œâ€œlifecycleâ€ï¼Œâ€œoverlayFSâ€ç­‰æ­£ç¡®æœ¯è¯­ï¼›
	â€¢	æ€è·¯æ¸…æ™°ï¼Œæ²¡æœ‰ç»•å¼¯æˆ–å†—ä½™ã€‚

â¸»

å»ºè®®ä½ å¯ä»¥è¿›ä¸€æ­¥ç»ƒä¹ çš„å¥å‹ï¼š
	â€¢	â€œIt acts as a bridge betweenâ€¦â€
	â€¢	â€œThis decouples container logic from backend-specific concernsâ€¦â€
	â€¢	â€œBy exposing a common interface, it allows containerd to remain backend-agnosticâ€¦â€

â¸»

ä½ è¦ä¸è¦æˆ‘æ¥ä¸‹æ¥æŠŠè¿™ä¸€æ®µå‘æˆå¸¦æ–­å¥æœ—è¯»ç”¨çš„ç‰ˆæœ¬ï¼Œæˆ–è€…ç»§ç»­è¿›å…¥ç¬¬å››æ¨¡å— â€”â€” 



éå¸¸å‡ºè‰²ï¼ä½ è¿™æ®µå›ç­”å·²ç»è¦†ç›–äº†å®Œæ•´çš„è®¾è®¡é€»è¾‘ï¼ŒåŒ…æ‹¬ï¼š
	â€¢	æ•°æ®é‡‡é›†ä¸åˆ†ç±»ï¼ˆmetrics / logs / digest-based analysisï¼‰
	â€¢	å†·çƒ­åˆ†å±‚ï¼ˆhot path caching + cold storage fallbackï¼‰
	â€¢	è·¯å¾„åˆ¤å®šæœºåˆ¶ä¸é€æ˜æ¢å¤é€»è¾‘
	â€¢	API ä¸å˜ã€é€»è¾‘åœ¨æœåŠ¡ç«¯æŠ½è±¡å±‚å®Œæˆ

æ¥ä¸‹æ¥æˆ‘å¸®ä½ æŠŠè¿™æ®µå†…å®¹ç»“æ„åŒ–æˆæ¸…æ™°å¯å¤ç”¨çš„ç³»ç»Ÿè®¾è®¡ç­”é¢˜æ¨¡ç‰ˆï¼Œé€‚ç”¨äº system reviewã€cost proposal æˆ– tech talkï¼š

â¸»

Design Answer: Reducing EBS IOPS via Blob Deduplication & Storage Tiering

â¸»

1. Diagnosis and Visibility

To verify that IOPS consumption is caused by repeated blob pulls:
	â€¢	Instrument digest tracking:
Inject logging and metrics at the blob fetch layer (e.g., TransferService or registry proxy) to track:
	â€¢	Digest of each pulled blob;
	â€¢	Access frequency per digest;
	â€¢	Time distribution of blob accesses (last 7 / 30 / 90 days);
	â€¢	Metrics examples:
	â€¢	blob_pull_total{digest=..., source="CI"}
	â€¢	blob_pull_unique_ratio{window="30d"}

â¸»

2. Optimization Design: Hot/Cold Tiering

Core Strategy:
	â€¢	Use digest frequency data to classify blobs into:
	â€¢	Hot blobs: Frequently accessed â†’ store in EBS or local cache/CDN;
	â€¢	Warm blobs: Occasionally accessed â†’ store in S3 standard;
	â€¢	Cold blobs: Rarely accessed â†’ move to S3 Glacier / IA;

Flow:

Client Pull Request
   â””â”€â”€ Check in local CDN or edge cache
       â””â”€â”€ If miss â†’ fallback to S3 Standard
           â””â”€â”€ If absent â†’ restore from Glacier (delayed path)

Note: The entire path uses the same blob digest-based addressing, no API change required on client side.

â¸»

3. Recovery Guarantee & Data Integrity
	â€¢	Use digest-based lookup to locate blob across tiers;
	â€¢	Maintain metadata registry to record which tier a blob is in;
	â€¢	On access, auto-trigger restoration if only Glacier copy exists;
	â€¢	Blob GC logic (via refcount or list) remains unchanged â€” separation of physical tiering vs logical GC.

â¸»

âœ… Summary Principles
	â€¢	No client change: All fallback logic is server-side;
	â€¢	Transparent fallback: Digest-driven redirection;
	â€¢	Metrics-aware GC: Clean + archive based on recent access stats;
	â€¢	Improved cost: Glacier for deep cold; EBS only for hot-path;

â¸»

å¦‚ä½ éœ€è¦æˆ‘æŠŠè¿™æ®µæ•´ç†æˆç³»ç»Ÿè®¾è®¡æ¼”ç¤ºç¨¿ã€markdown template æˆ–ç³»ç»Ÿæ¶æ„å›¾ï¼Œæˆ‘å¯ä»¥ç»§ç»­ç”Ÿæˆã€‚ä½ ä»Šå¤©å››æ¨¡å—å®Œæˆåº¦éå¸¸é«˜ï¼Œè¦ä¸è¦æˆ‘ç°åœ¨æ€»ç»“æ•´å¤©æ¨¡æ‹Ÿè¡¨ç°å¹¶ç”Ÿæˆç»ƒä¹ æ–‡æ¡£ï¼Ÿä½ è¯´ä¸€å¥æˆ‘å°±æ¥ã€‚