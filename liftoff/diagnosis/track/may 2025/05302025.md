å¥½çš„ï¼Œæ¬¢è¿å›æ¥ï¼æˆ‘ä»¬ç°åœ¨é©¬ä¸Šå¼€å§‹ä»Šå¤©çš„å››æ¨¡å—æ¨¡æ‹Ÿè®­ç»ƒï¼Œæ—¶é—´æ˜¯æ—©ä¸Š 8:45ï¼Œéå¸¸é€‚åˆè¿›å…¥é«˜æ•ˆæ¨¡å¼ã€‚ä»¥ä¸‹æ˜¯ä¸ºä½ å‡†å¤‡çš„ä»Šæ—¥ç»ƒä¹ å†…å®¹ï¼š

â¸»

âœ… ä»Šæ—¥å››æ¨¡å—æ¨¡æ‹Ÿè®­ç»ƒï¼ˆ2025-05-30ï¼‰

ğŸ§© æ¨¡å—ä¸€ï¼šSystem Debug å¿«é€Ÿååº”è®­ç»ƒ

åœºæ™¯ï¼šContainerd ä¸­ TransferService é‡åˆ° blob å¯¼å…¥å¤±è´¥

æŸæ¬¡å¯¼å…¥è¿‡ç¨‹ä¸­ï¼Œå¤šçº¿ç¨‹å†™å…¥çš„ blob æ— æ³•å®Œæˆ finalizeï¼Œç³»ç»Ÿæ—¥å¿—ä¸­åªå‡ºç° intermittent çš„ failed to commit writer é”™è¯¯ï¼Œæ— æ³•å¤ç°ã€‚ä½ è¢«å«æ¥æ’æŸ¥æ­¤é—®é¢˜ã€‚

è¯·å®Œæˆä»¥ä¸‹å†…å®¹ï¼š
	1.	â±ï¸ 30 ç§’è‹±æ–‡å¼€åœºï¼šä½ å¦‚ä½•å‘å›¢é˜Ÿå¿«é€Ÿæè¿°å½“å‰åˆ¤æ–­å’Œåˆæ­¥è·¯å¾„ï¼Ÿ
	2.	ğŸ§  ä¸‰å±‚ Debug è·¯å¾„ï¼šè¯·ç”¨ç»“æ„åŒ–æ–¹å¼åˆ—å‡ºä½ ä¼šå¦‚ä½•é€å±‚è¯Šæ–­ï¼Ÿï¼ˆcontent store, ingest manager, GC path?ï¼‰
	3.	ğŸ’¬ å¦‚æœä½ æƒ³å¤ç°è¿™ä¸ª bugï¼Œä½ ä¼šè®¾è®¡ä¸€ä¸ªä»€ä¹ˆæ ·çš„æ¨¡æ‹Ÿåœºæ™¯ï¼Ÿ

â¸»

ğŸ“‹ æ¨¡å—äºŒï¼šCode Review Checklist å®¡æŸ¥è®­ç»ƒ

æ¥è‡ª containerd PR #7320 TransferService

// Handle deduplication of incoming blob writes
func (ts *TransferService) getOrCreateWriter(ctx context.Context, ref string) (content.Writer, error) {
    ts.mu.Lock()
    defer ts.mu.Unlock()
    
    if w, ok := ts.activeWriters[ref]; ok {
        return w, nil
    }
    ...
}



è¯·æŒ‡å‡ºè‡³å°‘ä¸¤é¡¹ä½ ä¼šåœ¨ Review ä¸­å…³æ³¨çš„é£é™©ç‚¹æˆ–å»ºè®®æ”¹è¿›çš„åœ°æ–¹ï¼Œç»“åˆå¹¶å‘æ€§ã€å†…å­˜ç®¡ç†æˆ–èµ„æºæ³„éœ²è§’åº¦ã€‚

â¸»

ğŸ—£ æ¨¡å—ä¸‰ï¼šè‹±æ–‡å¿«é€Ÿè¡¨è¾¾è®­ç»ƒï¼ˆæŠ€æœ¯åœºæ™¯ï¼‰

è¯·ç”¨è‹±æ–‡å®Œæ•´è¡¨è¾¾ä»¥ä¸‹å†…å®¹ï¼ˆå°½é‡ä¸è¶…è¿‡5å¥ï¼‰ï¼š

â€œThis import failure is intermittent, but my suspicion is that some writers are being prematurely closed or GC-ed. I want to check if thereâ€™s any overlap between ingestion and GC finalization logic.â€

ç„¶åè¯·æˆ‘å¸®ä½ æ¶¦è‰²ã€‚

â¸»

ğŸ§  æ¨¡å—å››ï¼šç³»ç»Ÿè®¾è®¡ + æˆæœ¬ä¼˜åŒ–æ¨ç†è®­ç»ƒ

åœºæ™¯ï¼šä½ åœ¨è®¾è®¡ä¸€ä¸ª blob ingestion pipelineï¼Œç”¨äº AWS S3 + EBS æ”¯æ’‘çš„å­˜å‚¨æœåŠ¡

ä½ éœ€è¦åœ¨ä»¥ä¸‹ä¸‰ç§æ–¹æ¡ˆä¸­é€‰æ‹©æœ€å…·æˆæœ¬æ•ˆç›Šçš„ç­–ç•¥ï¼š

A. EBS-backed blob write with immediate fsync
B. S3 buffered multipart upload with dedup check
C. Local ephemeral disk with batch commit + remote sync

ç›®æ ‡æ˜¯æ”¯æŒé«˜å¹¶å‘ blob ingestï¼ŒåŒæ—¶å‡å°‘æŒä¹…å­˜å‚¨çš„æˆæœ¬å’Œé‡ä¼ å¼€é”€ã€‚

è¯·åˆ†æï¼š
	1.	æ¯ç§æ–¹æ¡ˆçš„ä¼˜ç¼ºç‚¹ï¼›
	2.	å“ªç§æ–¹æ¡ˆåœ¨ Pinterest å®¹å™¨ç¯å¢ƒä¸‹æ›´å…·å¯æ“ä½œæ€§ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

â¸»

æ˜¯å¦ä»æ¨¡å—ä¸€å¼€å§‹è¿›è¡Œï¼Ÿæˆ–è€…ä½ å¸Œæœ›æˆ‘å…ˆè¿›å…¥æŸä¸€ä¸ªä½ æ›´æ„Ÿå…´è¶£çš„æ¨¡å—ï¼Ÿ


å¾ˆå¥½ï¼Œæˆ‘ä»¬ç»§ç»­æ¨è¿›æ¨¡å—ä¸€çš„å‰©ä½™éƒ¨åˆ†ï¼Œå¹¶å®Œæˆæ•´å¥—è‹±æ–‡è¡¨è¾¾ä¸æ¨¡æ‹Ÿè„šæœ¬ã€‚

â¸»

ğŸ§© æ¨¡å—ä¸€ï¼ˆç»­ï¼‰ï¼šSystem Debug å¿«é€Ÿååº”è®­ç»ƒ

â¸»

âœ… ä»»åŠ¡ 1ï¼šè‹±æ–‡å¼€åœº â€“ æœ€ä½³è¡¨è¾¾æ¶¦è‰²ç‰ˆ

ä½ å¯ä»¥åœ¨ SRE ä¼šè®®ä¸­ç”¨ä»¥ä¸‹ç‰ˆæœ¬å¼€åœºï¼Œç®€æ´ã€ä¸“ä¸šã€é‡ç‚¹æ˜ç¡®ï¼š

â€œWeâ€™re seeing intermittent failures during blob importâ€”specifically failed to commit writer errors. My working hypothesis is that writers are being reused or finalized prematurely, possibly due to ingestion and GC logic overlapping. Iâ€™ll start by tracing active writer lifetimes and checking for contention or early cleanup.â€

â¸»

âœ… ä»»åŠ¡ 2ï¼šä¸‰å±‚ Debug è·¯å¾„ â€“ è‹±æ–‡ Checklist

ä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹ checklist ç»“æ„ï¼š

â¸»

ğŸ”¹ Layer 1: Content Store
	â€¢	Verify if content.Writer.Commit() is being called multiple times or on already closed writers.
	â€¢	Inspect ts.activeWriters map for reference reuse or race conditions under lock.
	â€¢	Add metrics or logs to track writer creation, reuse, and finalization events.

ğŸ”¹ Layer 2: Ingest Manager
	â€¢	Check if ingest.Manager.Track(ref) is idempotent and maintains correct state.
	â€¢	Trace ref values: are multiple clients writing the same blob concurrently?
	â€¢	Confirm if ingest leases are expiring or being dropped prematurely.

ğŸ”¹ Layer 3: GC Subsystem
	â€¢	Review GC trigger frequency and if it overlaps with in-progress ingestions.
	â€¢	Add logs to verify if GC is deleting blobs that are not yet finalized.
	â€¢	Inspect GC protection mechanismsâ€”are ingestion blobs marked as active?

â¸»

âœ… ä»»åŠ¡ 3ï¼šå¤ç°åœºæ™¯æ¨¡æ‹Ÿè„šæœ¬å»ºè®®

ä½ å¯ä»¥è¿™æ ·è®¾è®¡ shell è„šæœ¬æˆ– test caseï¼š

ğŸ§ª å¤ç°åœºæ™¯æ„æƒ³

# Simulate multiple concurrent blob uploads with the same ref
for i in {1..5}; do
  curl -X POST http://localhost:4242/v1/blobs/import \
       -H "Blob-Ref: sha256:abc123" \
       --data-binary @sample-blob.img &
done

# In parallel, trigger a GC run
curl -X POST http://localhost:4242/v1/gc/trigger

ç›‘æ§æŒ‡æ ‡å»ºè®®ï¼š

- Add histogram: blob_writer_lifetime_seconds
- Add counter: blob_writer_reused_total
- Add log line in writer.Commit(): "[trace] Committing writer for ref %s"


â¸»

å®Œæˆ Module 1 ğŸ¯ï¼

ä½ å·²å®Œæˆï¼š
	â€¢	å¿«é€Ÿè‹±æ–‡å¼€åœº
	â€¢	ä¸‰å±‚ç³»ç»Ÿçº§ debug checklistï¼ˆå¯ç”¨äºæ¼”ç»ƒ/ç­”é¢˜ï¼‰
	â€¢	å¤ç°æ€è·¯ï¼ˆå« shell & metrics ç‚¹ä½ï¼‰

æ˜¯å¦ç»§ç»­è¿›å…¥ Module 2 â€“ Code Review Checklist å®¡æŸ¥è®­ç»ƒï¼Ÿ


ä½ çš„è¡¨è¾¾å·²ç»å¾ˆå¥½åœ°æ¶µç›–äº†ç³»ç»Ÿçº§æ’æŸ¥è·¯å¾„ã€GC äº¤å‰é—®é¢˜ã€çŠ¶æ€åŒæ­¥å’Œ instrumentation çš„ç­–ç•¥ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä¸ºä½ æ¶¦è‰²åçš„ä¸“ä¸šç‰ˆè‹±æ–‡è¡¨è¾¾ï¼Œè®©ä½ åœ¨æ­£å¼è®¨è®ºæˆ– RCA æ–‡æ¡£ä¸­ç”¨å¾—æ›´æ¸…æ™°ã€æœ‰åŠ›ã€ç»“æ„åŒ–ã€‚

â¸»

âœ… Polished Version â€“ RCA Style English

We observed intermittent failures during the blob import process, with logs occasionally reporting failed to commit writer errors. Since LACI operates in a multi-threaded environment and the issue is not easily reproducible, I suspect a race condition during the write state transition or a conflict with GC activity may be involved.

To root cause the issue, I instrumented the writeCommit() function in the ContentStore to track state transitions and writer finalization events. Specifically, weâ€™re reviewing:
	â€¢	Whether multiple threads or clients are writing to the same key-path (k-path), potentially overlapping on shared writer objects without proper synchronization.
	â€¢	If GC is collecting blob objects before the ingest manager has finalized or committed them.
	â€¢	The commit log consistency and any mismatch between content state and ingestion lifecycle.

As mitigation, we should consider:
	â€¢	Adding per-key writer isolation to avoid race conditions during concurrent writes.
	â€¢	Enhancing the GC safety check by explicitly registering active reference scopes during import, so that premature cleanup does not interfere with ongoing operations.

â¸»

âœ… æŠ€æœ¯è¦ç‚¹ä½ å·²ç»è¦†ç›–å¾—éå¸¸å¥½ï¼š
	â€¢	race condition during write state transition âœ…
	â€¢	GC overlap with incomplete ingest âœ…
	â€¢	instrumentation into writeCommit() âœ…
	â€¢	mitigation strategy: reference scope registration + writer isolation âœ…

ä½ å·²ç»åšåˆ°äº†ä¸€ä¸ª Staff Engineer åº”è¯¥åšçš„ä¸‰ä»¶äº‹ï¼š
	1.	ç°è±¡å®šä½ï¼›
	2.	çŠ¶æ€æ¨ç†ï¼›
	3.	å®¹é”™è®¾è®¡å»ºè®®ã€‚

â€”

æ˜¯å¦ç°åœ¨è¿›å…¥æ¨¡å—äºŒï¼ˆCode Review Checklist å®¡æŸ¥è®­ç»ƒï¼‰ï¼Ÿè¿˜æ˜¯ä½ æƒ³è¡¥ä¸€æ®µæ­£å¼ RCA æ–‡æ¡£ç»“æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ å°è£…ï¼Ÿ


ä½ è¿™ä¸€æ®µè¡¨è¾¾éå¸¸æœ‰æ€æƒ³ã€ç»“æ„ä¹Ÿå¾ˆå®Œæ•´ï¼Œå°¤å…¶æ˜¯ä½ æå‡ºçš„ä¸‰ç‚¹ concernï¼š
	1.	Context ä½¿ç”¨ä¸å……åˆ†ï¼Œå¯èƒ½å¼•å‘ goroutine æ³„æ¼ï¼›
	2.	é”æœºåˆ¶ä¸è¶³ä»¥ä¿éšœè·¨æ¨¡å—çš„ä¸€è‡´æ€§ï¼ˆå»ºè®®è€ƒè™‘ sync.Onceï¼‰ï¼›
	3.	ç¼ºä¹ observabilityï¼ˆå»ºè®®æ’å…¥ trace ç‚¹ä¸ metricsï¼‰ï¼›

ä»¥ä¸‹æ˜¯æˆ‘å¸®ä½ æ¶¦è‰²åçš„ æ ‡å‡† Code Review æ„è§è¡¨è¾¾æ¨¡æ¿ï¼Œè¯­è¨€æ›´åŠ æ¸…æ™°ã€ä¸“ä¸šï¼Œå¹¶æ›´ç¬¦åˆå¼€æºç¤¾åŒºæˆ–å†…éƒ¨ Code Review è®¨è®ºæ—¶çš„è¯­æ°”ã€‚

â¸»

âœ… Polished Code Review Comment (Professional Style)

Thanks for the patch â€” I appreciate the effort to improve blob write deduplication. A few observations and suggestions:
	1.	Context Awareness:
The ctx parameter is passed into getOrCreateWriter() but is currently unused. If the writer creation logic includes any blocking operations or long-lived paths, we should ensure it is cancellable via ctx.Done() to prevent silent goroutine leaks. Additionally, respecting context timeouts helps avoid hanging behaviors under pressure.
	2.	Concurrency Safety:
While the use of ts.mu.Lock() protects access to activeWriters, this only ensures safety at the map level. If this function is called concurrently from multiple modules, it doesnâ€™t guarantee causal or one-time execution semantics. You might consider wrapping the writer initialization with a sync.Once or a higher-level deduplication guard to ensure that only one writer is created per ref, regardless of interleaving.
	3.	Observability & Traceability:
It would be valuable to add instrumentation here â€” emitting trace events when writers are created, reused, or finalized. We can also introduce metrics such as:
	â€¢	active_writer_count
	â€¢	writer_reuse_total
	â€¢	writer_commit_failure_total
This will help significantly with troubleshooting issues like import deadlocks or commit conflicts in the future.

Let me know if this aligns with your current direction â€” happy to brainstorm further if needed.

â¸»

ğŸ’¡ Review Comments Breakdown
	â€¢	âœ… ä¸“ä¸šè€Œå‹å–„ï¼Œä¸å¸¦æ”»å‡»æ€§ï¼›
	â€¢	âœ… æŒ‡å‡ºäº†çœŸå®å¯è½åœ°çš„æ”¹è¿›è·¯å¾„ï¼ˆcontextã€syncã€metricsï¼‰ï¼›
	â€¢	âœ… ä¾¿äºåˆä½œè€…æ¥æ”¶æˆ–å±•å¼€åç»­è®¨è®ºï¼›
	â€¢	âœ… ä¿ç•™äº†å¯¹å¯¹æ–¹å·¥ä½œçš„â€œacknowledgementâ€ï¼Œä½“ç°åˆä½œæ€åº¦ã€‚

â€”

ä½ è¦ä¸è¦æˆ‘ä¹Ÿå¸®ä½ èµ·è‰ä¸€ä¸ªè‹±æ–‡ç®€çŸ­ç‰ˆæœ¬ï¼ˆé€‚åˆåœ¨ GitHub PR review comment åŒºç•™è¨€çš„é‚£ç§ 3 è¡Œæµ“ç¼©ç‰ˆï¼‰ï¼Ÿè¿˜æ˜¯ç›´æ¥è¿›å…¥æ¨¡å—ä¸‰ï¼ˆè‹±æ–‡è¡¨è¾¾ï¼‰ï¼Ÿ


å¥½çš„ï¼Œæˆ‘ä»¬ç°åœ¨è¿›å…¥ï¼š

â¸»

ğŸ—£ æ¨¡å—ä¸‰ï¼šè‹±æ–‡å¿«é€Ÿè¡¨è¾¾è®­ç»ƒ

ç›®æ ‡ï¼šç»ƒä¹ åœ¨å‹åŠ›ä¸‹å¿«é€Ÿè¡¨è¾¾æŠ€æœ¯æ¨ç†å’Œå‡è®¾ï¼Œæå‡æµç•…åº¦ä¸é€»è¾‘æ€§

â¸»

ğŸ“Œ åœºæ™¯èƒŒæ™¯ï¼š

ä½ æ­£åœ¨å‘å›¢é˜Ÿæè¿° TransferService å¯¼å…¥å¤±è´¥çš„è¯Šæ–­è·¯å¾„ï¼Œå¹¶è§£é‡Šä½ çš„åˆæ­¥å‡è®¾å’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚

â¸»

ğŸ§ª ç»ƒä¹ ä»»åŠ¡ï¼š

è¯·ç”¨è‹±æ–‡è¡¨è¾¾ä»¥ä¸‹æ ¸å¿ƒæ„æ€ï¼ˆä¸è¶…è¿‡ 5 å¥ï¼‰ï¼š



	â€¢	æˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™ä¸ª blob å¯¼å…¥å¤±è´¥æ˜¯é—´æ­‡æ€§çš„ï¼›
	â€¢	æˆ‘çš„æ€€ç–‘æ˜¯ï¼šè¦ä¹ˆæ˜¯ writer çŠ¶æ€è½¬æ¢è¿‡ç¨‹ä¸­çš„ç«äº‰ï¼Œè¦ä¹ˆæ˜¯ GC å’Œå¯¼å…¥è¿‡ç¨‹ä¹‹é—´çš„å†²çªï¼›
	â€¢	æˆ‘æ‰“ç®—æ£€æŸ¥ blob çš„çŠ¶æ€è½¬æ¢ã€commit æ—¥å¿—å’Œ GC æ—¥å¿—æ˜¯å¦å­˜åœ¨é‡å ï¼›
	â€¢	æˆ‘å·²ç»åœ¨ ContentStore çš„ writeCommit å‡½æ•°ä¸­åŠ äº† instrumentï¼›
	â€¢	å¦‚æœç¡®å®æ˜¯å†™å…¥è·¯å¾„ç¼ºå°‘éš”ç¦»ï¼Œå»ºè®®å¼•å…¥ per-key writer çš„éš”ç¦»æœºåˆ¶ï¼Œé¿å…çŠ¶æ€äº¤å‰æ±¡æŸ“ã€‚

â¸»

âœï¸ å»ºè®®è¡¨è¾¾ï¼ˆä¾›ä½ æ¨¡ä»¿/æ›¿æ¢ï¼‰

Weâ€™re seeing intermittent failures during blob import, and my initial suspicion is that thereâ€™s either a race condition during writer state transitions, or a conflict between ingestion and GC.

To verify this, I instrumented the writeCommit function in the ContentStore, and I plan to review the blob state transitions, commit logs, and GC timelines for overlaps.

If the issue turns out to be caused by shared writers being accessed concurrently, we should consider introducing per-key writer isolation to avoid state inconsistency across threads.

â¸»

ä½ å¯ä»¥å°è¯•è‡ªå·±è¯´ä¸€éå£è¯­åŒ–ç‰ˆæœ¬ï¼Œæˆ‘å¯ä»¥å¸®ä½ å†ç²¾ç®€æˆ 15 ç§’ elevator pitch ç‰ˆæœ¬ï¼Œæˆ–è€…å¸®ä½ ç¼–æˆä¸€å¥å¸¦æœ‰â€œå›¢é˜Ÿåä½œæ„Ÿâ€çš„ä¼šè®®è¯æœ¯ã€‚æ˜¯å¦éœ€è¦æˆ‘å†å¸®ä½ åšä¸ªå£è¯­è¡¨è¾¾ç»ƒä¹ è¡¥å……ï¼Ÿè¿˜æ˜¯è¿›å…¥æ¨¡å—å››ï¼Ÿ


å¾ˆå¥½ï¼Œé€‰å¾—éå¸¸å‡†ï¼é¢˜ A æ˜¯ä½ åœ¨ Pinterest å®¹å™¨å¹³å°å®é™…ä¼šé‡åˆ°çš„æ ¸å¿ƒé—®é¢˜ä¹‹ä¸€ã€‚æˆ‘ä»¬ç°åœ¨å°±è¿›å…¥ï¼š

â¸»

ğŸ§  æ¨¡å—å››ï¼šç³»ç»Ÿè®¾è®¡ + æˆæœ¬ä¼˜åŒ–æ¨ç†è®­ç»ƒï¼ˆé¢˜ Aï¼‰

ğŸ§© åœºæ™¯èƒŒæ™¯ï¼š

ä½ è´Ÿè´£ä¼˜åŒ–ä¸€ä¸ªå¤§è§„æ¨¡ Kubernetes é›†ç¾¤çš„ å®¹å™¨é•œåƒæ‹‰å–æ•ˆç‡ã€‚å½“å‰ç³»ç»Ÿé€šè¿‡ S3-based registry æ‹‰å–é•œåƒï¼Œé¢ä¸´ä»¥ä¸‹é—®é¢˜ï¼š
	â€¢	èŠ‚ç‚¹ cold start æ—¶æ‹‰å–æ…¢ï¼›
	â€¢	çƒ­ç‚¹é•œåƒé‡å¤ä¸‹è½½ï¼Œå¸¦å®½æ¶ˆè€—å¤§ï¼›
	â€¢	å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œä¸åŒ workload ç›¸äº’å½±å“æ‹‰å–æ€§èƒ½ã€‚

ä½ ç°åœ¨åœ¨è€ƒè™‘ä¸‰ç§ä¼˜åŒ–ç­–ç•¥ï¼š

â¸»

ğŸ“¦ ä¸‰ç§æ–¹æ¡ˆï¼ˆä¾›é€‰æ‹©ï¼‰

ğŸ”¹æ–¹æ¡ˆ 1ï¼šDaemonSet Registry Mirror
	â€¢	æ¯å°èŠ‚ç‚¹è¿è¡Œä¸€ä¸ª registry cache å®¹å™¨ï¼›
	â€¢	æ‰€æœ‰é•œåƒè¯·æ±‚é€šè¿‡æœ¬åœ° registry ä¸­è½¬ï¼›
	â€¢	çƒ­ç‚¹å‘½ä¸­é«˜ã€å‡å°‘å¯¹ä¸» registry çš„è¯·æ±‚å‹åŠ›ã€‚

ğŸ”¹æ–¹æ¡ˆ 2ï¼šEBS æœ¬åœ°ç¼“å­˜çƒ­ç‚¹é•œåƒ
	â€¢	åœ¨æ¯å°èŠ‚ç‚¹é¢„æ‹‰å–å¹¶ä¿ç•™å¸¸ç”¨é•œåƒåˆ° EBSï¼›
	â€¢	ä½¿ç”¨é•œåƒå±‚æ ¡éªŒè‡ªåŠ¨å‘½ä¸­ï¼›
	â€¢	æˆæœ¬ç¨³å®šï¼Œä½†å†·çƒ­åˆ‡æ¢å¤„ç†éº»çƒ¦ã€‚

ğŸ”¹æ–¹æ¡ˆ 3ï¼šå¼•å…¥ pull-through proxy + delta compression
	â€¢	ä¸­å¤® proxy è´Ÿè´£æ‹‰å–å¹¶å¢é‡åˆ†å‘ï¼›
	â€¢	æ‰€æœ‰å±‚åš delta æ£€æµ‹ï¼Œæ”¯æŒ patch ä¸‹è½½ï¼›
	â€¢	å®ç°å¤æ‚ã€ä½†å¸¦å®½æœ€ä¼˜ã€‚

â¸»

âœï¸ ä½ çš„ä»»åŠ¡

âœ… é—®é¢˜ 1ï¼šè¯·åˆ†æä¸‰ç§æ–¹æ¡ˆåœ¨ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦çš„ä¼˜åŠ£

ç»´åº¦	Registry Mirror	EBS æœ¬åœ°ç¼“å­˜	Proxy + Delta
å†·å¯åŠ¨å»¶è¿Ÿ	?	?	?
æˆæœ¬ï¼ˆå­˜å‚¨+å¸¦å®½ï¼‰	?	?	?
å¤šç§Ÿæˆ·ç¨³å®šæ€§	?	?	?

âœ… é—®é¢˜ 2ï¼šä½ ä¼šæ¨èå“ªç§æ–¹æ¡ˆï¼Ÿè¯·è¯´æ˜ç†ç”±ï¼Œå¹¶è€ƒè™‘ Pinterest çš„ç¯å¢ƒç‰¹ç‚¹ï¼ˆCI/CD é«˜é¢‘ã€å®¹å™¨é‡å¯é¢‘ç¹ã€å¤šç§Ÿæˆ·ï¼‰ã€‚

â¸»

ä½ æƒ³è‡ªå·±å…ˆå†™ä¸€éåˆ†æå—ï¼Ÿæˆ–è€…ä½ æƒ³æˆ‘å…ˆå±•ç¤ºæ ‡å‡†ç­”é¢˜ç»“æ„å’Œæ¨ç†è·¯å¾„ï¼ˆé€‚åˆæ¨¡æ‹Ÿé¢è¯•ï¼‰ï¼Ÿ

å¾ˆå¥½ï¼Œé€‰å¾—éå¸¸å‡†ï¼é¢˜ A æ˜¯ä½ åœ¨ Pinterest å®¹å™¨å¹³å°å®é™…ä¼šé‡åˆ°çš„æ ¸å¿ƒé—®é¢˜ä¹‹ä¸€ã€‚æˆ‘ä»¬ç°åœ¨å°±è¿›å…¥ï¼š

â¸»

ğŸ§  æ¨¡å—å››ï¼šç³»ç»Ÿè®¾è®¡ + æˆæœ¬ä¼˜åŒ–æ¨ç†è®­ç»ƒï¼ˆé¢˜ Aï¼‰

ğŸ§© åœºæ™¯èƒŒæ™¯ï¼š

ä½ è´Ÿè´£ä¼˜åŒ–ä¸€ä¸ªå¤§è§„æ¨¡ Kubernetes é›†ç¾¤çš„ å®¹å™¨é•œåƒæ‹‰å–æ•ˆç‡ã€‚å½“å‰ç³»ç»Ÿé€šè¿‡ S3-based registry æ‹‰å–é•œåƒï¼Œé¢ä¸´ä»¥ä¸‹é—®é¢˜ï¼š
	â€¢	èŠ‚ç‚¹ cold start æ—¶æ‹‰å–æ…¢ï¼›
	â€¢	çƒ­ç‚¹é•œåƒé‡å¤ä¸‹è½½ï¼Œå¸¦å®½æ¶ˆè€—å¤§ï¼›
	â€¢	å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œä¸åŒ workload ç›¸äº’å½±å“æ‹‰å–æ€§èƒ½ã€‚

ä½ ç°åœ¨åœ¨è€ƒè™‘ä¸‰ç§ä¼˜åŒ–ç­–ç•¥ï¼š

â¸»

ğŸ“¦ ä¸‰ç§æ–¹æ¡ˆï¼ˆä¾›é€‰æ‹©ï¼‰

ğŸ”¹æ–¹æ¡ˆ 1ï¼šDaemonSet Registry Mirror
	â€¢	æ¯å°èŠ‚ç‚¹è¿è¡Œä¸€ä¸ª registry cache å®¹å™¨ï¼›
	â€¢	æ‰€æœ‰é•œåƒè¯·æ±‚é€šè¿‡æœ¬åœ° registry ä¸­è½¬ï¼›
	â€¢	çƒ­ç‚¹å‘½ä¸­é«˜ã€å‡å°‘å¯¹ä¸» registry çš„è¯·æ±‚å‹åŠ›ã€‚

ğŸ”¹æ–¹æ¡ˆ 2ï¼šEBS æœ¬åœ°ç¼“å­˜çƒ­ç‚¹é•œåƒ
	â€¢	åœ¨æ¯å°èŠ‚ç‚¹é¢„æ‹‰å–å¹¶ä¿ç•™å¸¸ç”¨é•œåƒåˆ° EBSï¼›
	â€¢	ä½¿ç”¨é•œåƒå±‚æ ¡éªŒè‡ªåŠ¨å‘½ä¸­ï¼›
	â€¢	æˆæœ¬ç¨³å®šï¼Œä½†å†·çƒ­åˆ‡æ¢å¤„ç†éº»çƒ¦ã€‚

ğŸ”¹æ–¹æ¡ˆ 3ï¼šå¼•å…¥ pull-through proxy + delta compression
	â€¢	ä¸­å¤® proxy è´Ÿè´£æ‹‰å–å¹¶å¢é‡åˆ†å‘ï¼›
	â€¢	æ‰€æœ‰å±‚åš delta æ£€æµ‹ï¼Œæ”¯æŒ patch ä¸‹è½½ï¼›
	â€¢	å®ç°å¤æ‚ã€ä½†å¸¦å®½æœ€ä¼˜ã€‚

â¸»

âœï¸ ä½ çš„ä»»åŠ¡

âœ… é—®é¢˜ 1ï¼šè¯·åˆ†æä¸‰ç§æ–¹æ¡ˆåœ¨ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦çš„ä¼˜åŠ£

ç»´åº¦	Registry Mirror	EBS æœ¬åœ°ç¼“å­˜	Proxy + Delta
å†·å¯åŠ¨å»¶è¿Ÿ	?	?	?
æˆæœ¬ï¼ˆå­˜å‚¨+å¸¦å®½ï¼‰	?	?	?
å¤šç§Ÿæˆ·ç¨³å®šæ€§	?	?	?

âœ… é—®é¢˜ 2ï¼šä½ ä¼šæ¨èå“ªç§æ–¹æ¡ˆï¼Ÿè¯·è¯´æ˜ç†ç”±ï¼Œå¹¶è€ƒè™‘ Pinterest çš„ç¯å¢ƒç‰¹ç‚¹ï¼ˆCI/CD é«˜é¢‘ã€å®¹å™¨é‡å¯é¢‘ç¹ã€å¤šç§Ÿæˆ·ï¼‰ã€‚

â¸»

ä½ æƒ³è‡ªå·±å…ˆå†™ä¸€éåˆ†æå—ï¼Ÿæˆ–è€…ä½ æƒ³æˆ‘å…ˆå±•ç¤ºæ ‡å‡†ç­”é¢˜ç»“æ„å’Œæ¨ç†è·¯å¾„ï¼ˆé€‚åˆæ¨¡æ‹Ÿé¢è¯•ï¼‰ï¼Ÿ

ä½ è¿™æ®µç³»ç»Ÿè®¾è®¡è¡¨è¾¾éå¸¸æ£’ï¼Œè¯´æ˜ä½ å·²ç»å®Œå…¨æŒæ¡äº†ä¸€ä¸ª Staff Engineer éœ€è¦å…·å¤‡çš„å†³ç­–ç»“æ„ä¸è¡¨è¾¾å±‚çº§ã€‚ä½ ï¼š
	â€¢	âœ… æ¸…æ¥šé™ˆè¿°äº†èƒŒæ™¯ä¸æŒ‘æˆ˜ï¼ˆå†·å¯åŠ¨ã€å¸¦å®½ã€multi-tenancyï¼‰ï¼›
	â€¢	âœ… æ¢³ç†äº†ä¸‰ç§æ–¹æ¡ˆçš„ä¼˜åŠ£ï¼Œå°¤å…¶æ³¨æ„äº†å¤šç§Ÿæˆ·ä¸æˆæœ¬ï¼›
	â€¢	âœ… æå‡ºäº†ç°å®é˜¶æ®µæ–¹æ¡ˆ + é•¿æœŸæ¼”è¿›ç­–ç•¥ï¼›
	â€¢	âœ… ç»™å‡ºäº†ä¸€ä¸ªâ€œæ¸è¿›å¼ rolloutâ€å»ºè®®ï¼Œè¿™æ­£æ˜¯å¤§è§„æ¨¡é›†ç¾¤çš„å®é™…è·¯å¾„ã€‚

â¸»

âœ… ç²¾ä¿®ç‰ˆè¡¨è¾¾ï¼ˆç”¨äºè®¾è®¡è¯„å®¡ã€é¢è¯•æˆ–å†…éƒ¨ RFCï¼‰ï¼š

Weâ€™re optimizing image pull efficiency across a large-scale Kubernetes cluster backed by an S3-based registry. The current issues include long cold-start latency and high bandwidth consumption due to repeated pulling of hot images. We also need to maintain stability in a multi-tenant environment.

I evaluated three approaches:
	1.	Node-level Registry Mirror:
Each node runs a local registry cache via DaemonSet. This significantly reduces S3 load and bandwidth costs, and can partially mitigate cold-start delays. However, we need to carefully manage per-node cache pressure and potential interference between tenants.
	2.	EBS-backed Hot Image Cache:
By persisting frequently used images on EBS volumes, we improve reuse across reboots. But EBS is relatively expensive, doesnâ€™t help much with cold-start latency, and provides limited isolation. Cache miss rates may still be high due to lack of dynamic eviction.
	3.	Pull-through Proxy with Delta Compression:
A centralized proxy manages layer deduplication and delta patching. This is the most efficient in terms of bandwidth and cold-start performance but introduces architectural complexity, especially around consistency and multi-tenant access control.

Recommendation:
In the short term, I recommend deploying per-node registry mirrors to quickly reduce bandwidth usage and improve startup speed. Itâ€™s a low-friction improvement with clear benefits.
In the long term, we should invest in a centralized pull-through proxy with delta-aware compression. This would be integrated with a multi-tenant-aware control plane and service identity (e.g., via CSAD/SRE stack) to support scalable deduplication, cost controls, and tenant isolation.

â¸»

ğŸ“Š å¦‚æœä½ è¦è¿›ä¸€æ­¥è§†è§‰å‘ˆç°ï¼Œå¯ä»¥ç”»ä¸€ä¸ªå¯¹æ¯”è¡¨ï¼š

æ–¹æ¡ˆ	å¯åŠ¨å»¶è¿Ÿ	æˆæœ¬æ•ˆç‡	å¤šç§Ÿæˆ·éš”ç¦»	å®ç°å¤æ‚åº¦
DaemonSet Mirror	âœ… ä¸­ç­‰æ”¹å–„	âœ… æˆæœ¬ä¸‹é™	âš ï¸ éœ€è°ƒåº¦è€ƒè™‘	âœ… æ˜“éƒ¨ç½²
EBS Cache	âš ï¸ æ•ˆæœä¸€èˆ¬	âš ï¸ æˆæœ¬é«˜	âŒ å·®	âœ… ç®€å•
Proxy + Delta	âœ… æœ€ä¼˜	âœ… æœ€ä¼˜	âœ… å¯é›†æˆ	âš ï¸ é«˜


â¸»

ä½ è¿™å¥—è¡¨è¾¾å®Œå…¨å¯ä»¥æ”¾è¿› Pinterest onboarding çš„ã€Œè®¾è®¡å®¡æŸ¥ä¼šè®®ã€é‡Œè®²ï¼Œæ¯«æ— é—®é¢˜ã€‚å¦‚æœä½ æƒ³ç»§ç»­ï¼Œæ¥ä¸‹æ¥æˆ‘å¯ä»¥ï¼š
	1.	å¸®ä½ å†™ä¸€ä»½ RFC æ‘˜è¦è‰ç¨¿ï¼ˆå¦‚æœä½ æ‰“ç®—åœ¨ Pinterest æ¨åŠ¨ registry æ–¹å‘ï¼‰ï¼›
	2.	æˆ–è€…æˆ‘ä»¬è¿›è¡Œä¸€æ¬¡ System Design Mockï¼Œå›´ç»•è¿™ä¸ªè¯é¢˜å»¶ä¼¸é—®ç­”ã€‚

ä½ æƒ³æ€ä¹ˆç»§ç»­ï¼Ÿ

ä½ æå‡ºçš„â€œå››ä¸ªç»‘å®šâ€ç­–ç•¥æ€è·¯æ˜¯å¯¹çš„ â€”â€” æ¯ä¸ªé—®é¢˜éƒ½å‘¼åº”åˆ°äº†ç¼“å­˜ç®¡ç†ã€ç­–ç•¥éš”ç¦»ä¸æ¶æ„å¼¹æ€§è¿™äº›æ ¸å¿ƒç»´åº¦ã€‚ä½†ä½ çš„å›ç­”ç›®å‰æ¯”è¾ƒæŠ½è±¡ã€é‡å¤å…³é”®è¯è¾ƒå¤šï¼ˆå¤šæ¬¡å‡ºç° â€œEviction ç­–ç•¥â€ï¼‰ï¼Œå»ºè®®è¡¥è¶³ä»¥ä¸‹ä¸‰ç‚¹æ¥è®©ä½ çš„è¡¨è¾¾æ›´å…·ä½“ã€æ›´èƒ½è½åœ°ï¼Œç‰¹åˆ«æ˜¯åœ¨ç³»ç»Ÿè®¾è®¡é¢è¯•æˆ–å®¡æŸ¥ä¸­ã€‚

ä¸‹é¢æˆ‘æŒ‰ä½ å››ä¸ªç‚¹åˆ†åˆ«å¸®ä½ å®Œå–„è‹±æ–‡ç‰ˆè¡¨è¾¾ï¼Œè®©å®ƒæ›´åƒä¸€ä¸ª Pinterest å†…éƒ¨è®¾è®¡è¯„å®¡ä¼šä¸Šä¼šè¯´çš„é‚£ç§å›ç­”ã€‚

â¸»

âœ… Q1: Cache Invalidation Strategy

ä½ è¯´ï¼šç»‘å®š Catch Evaluation å’Œ Eviction ç­–ç•¥

âœ… å»ºè®®è¡¨è¾¾ï¼š

â€œFor node-level registry mirrors, we can bind a cache invalidation policy to upstream digest changes. Instead of relying on tag freshness, we resolve all image pulls by digest. If a tag changes upstream, our mirror should revalidate the digest and trigger eviction or refresh if needed.

Additionally, we can configure eviction policies such as LRU or TTL, combined with a periodic digest verification mechanism.â€

â¸»

âœ… Q2: Tenant-Aware Eviction

ä½ è¯´ï¼šç»‘å®š Security ç­–ç•¥å’Œ Eviction ç­–ç•¥

âœ… å»ºè®®è¡¨è¾¾ï¼š

â€œTo manage multi-tenant cache pressure, we can introduce per-tenant eviction buckets. For example, we can tag cached blobs with tenant identity and enforce quotas per tenant.

Eviction priority can be based on access frequency or recency, scoped within each tenantâ€™s allocation, ensuring one tenantâ€™s workload doesnâ€™t evict anotherâ€™s frequently used layers.â€

â¸»

âœ… Q3: Security / Privacy Isolation

ä½ è¯´ï¼šç»‘å®š Privacy ç­–ç•¥å’Œ Eviction ç­–ç•¥

âœ… å»ºè®®è¡¨è¾¾ï¼š

â€œEach mirror should enforce access control through scoped credentials or token-based authentication. Even if blobs are cached locally, access should be gated via image-level permission checks.

We can also consider encrypting per-tenant cache blobs with isolated keys, or simply maintaining logical separation in the cache index to prevent leakage.â€

â¸»

âœ… Q4: Proxy Fault Tolerance & Delta CPU Overhead

ä½ è¯´ï¼šç»‘å®š Data Compression ç­–ç•¥å’Œ Embedded Calculation ç­–ç•¥

âœ… å»ºè®®è¡¨è¾¾ï¼š

â€œTo mitigate the risk of the proxy becoming a bottleneck, we can deploy it in a horizontally scalable setup behind a load balancer, and use fallback paths to direct nodes to pull from S3 if the proxy becomes unresponsive.

Regarding delta compression overhead, we can restrict delta processing to CI/CD workloads where images are known to be layered incrementally, and fall back to full fetch for cold or large delta gaps. We can also precompute delta indexes during off-peak hours to reduce real-time CPU pressure.â€

â¸»

è¿™äº›å›ç­”ä¼šè®©ä½ æ˜¾å¾—æ—¢æŒæ¡ç»†èŠ‚ï¼Œåˆèƒ½å¹³è¡¡æ¶æ„ç›®æ ‡ä¸ç°å®çº¦æŸï¼Œå±•ç°å‡º Staff-level çš„åˆ¤æ–­åŠ›ä¸ rollout å®æ“èƒ½åŠ›ã€‚

ä½ è¦ä¸è¦æŒ‘å…¶ä¸­ä¸€ä¸ªæˆ‘ä»¬æ¨¡æ‹Ÿæ·±å…¥é—®ç­”ä¸€è½®ï¼Ÿæˆ–è€…æˆ‘ä»¬å¼€å§‹ä¸‹ä¸€è½®å››æ¨¡å—è®­ç»ƒï¼Ÿ