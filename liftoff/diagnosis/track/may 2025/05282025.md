å¥½ï¼Œç«‹åˆ»ä¸ºä½ ç”Ÿæˆä»Šå¤©çš„ å››æ¨¡å—æ¨¡æ‹Ÿè®­ç»ƒï¼ˆ2025-05-27ï¼‰ï¼Œç¡®ä¿å†…å®¹å…¨æ–°ã€å®æˆ˜æ€§å¼ºã€è¦†ç›–è¡¨è¾¾ä¸åˆ¤æ–­åŠ›ï¼š

â¸»

ğŸ§  æ¨¡å—ä¸€ï¼šSystem Debug å¿«é€Ÿååº”è®­ç»ƒ

ğŸ¯ åœºæ™¯æ ‡é¢˜ï¼šå®¹å™¨ CPU ä½¿ç”¨ç‡é£™é«˜ä½†æ²¡æœ‰è´Ÿè½½è¿›ç¨‹

ä½ è´Ÿè´£çš„å®¹å™¨é›†ç¾¤ä¸­æŸä¸ªèŠ‚ç‚¹é¢‘ç¹æŠ¥è­¦ï¼Œæç¤ºæŸäº›å®¹å™¨çš„ CPU usage çªç„¶é£™å‡è‡³ 90% ä»¥ä¸Šã€‚ä½†ä½ ç™»å½•èŠ‚ç‚¹åå´å‘ç° top æˆ– htop çœ‹ä¸åˆ°æ˜æ˜¾é«˜å ç”¨è¿›ç¨‹ï¼Œè€Œå®¹å™¨æœ¬èº«ä¸šåŠ¡æ­£å¸¸ï¼Œè®¿é—®æ— å¼‚å¸¸ã€‚

ä»»åŠ¡ï¼š
	1.	è¯·ç”¨è‹±æ–‡å®Œæˆ 30 ç§’å¼€åœºè¡¨è¾¾ï¼ˆåˆ¤æ–­ç°è±¡ + è°ƒæŸ¥è·¯å¾„ï¼‰ã€‚
	2.	åˆ—å‡ºä½ ä¼šä»å“ª 3â€“5 å±‚ç³»ç»Ÿç»“æ„å…¥æ‰‹è°ƒæŸ¥ï¼Œå¹¶è¯´æ˜æ¯å±‚çš„å·¥å…·æˆ–å‘½ä»¤ã€‚
	3.	ç»™å‡ºå¯èƒ½ root cause çš„ 3 ç§çŒœæµ‹ï¼Œå¹¶è¯´æ˜å¦‚ä½•éªŒè¯ã€‚

â¸»

âœ… æ¨¡å—äºŒï¼šCode Review Checklist è®­ç»ƒ

ğŸ¯ ä»£ç ä¸»é¢˜ï¼šGolang ä¸­ context + goroutine æ³„éœ²

func HandleRequest(ctx context.Context, wg *sync.WaitGroup) {
    wg.Add(1)
    go func() {
        defer wg.Done()
        for {
            select {
            case <-ctx.Done():
                return
            default:
                time.Sleep(1 * time.Second)
            }
        }
    }()
}

ä»»åŠ¡ï¼š
	1.	ä½ åœ¨ review æ—¶ä¼šæŒ‡å‡ºå“ªäº›é—®é¢˜ï¼Ÿè¯·åˆ—å‡ºè‡³å°‘ 3 æ¡ Code Review æ„è§ã€‚
	2.	å“ªäº›è®¾è®¡å¯èƒ½å¯¼è‡´ goroutine æ³„éœ²ï¼Ÿå¦‚ä½•ä¿®å¤ï¼Ÿ
	3.	å¦‚æœè¿™æ˜¯ containerd çš„æŸä¸ª plugin é‡Œå‡ºç°çš„ä»£ç ï¼Œä½ ä¼šå»ºè®®åŠ å…¥å“ªäº› metric æˆ–æ—¥å¿—ï¼Ÿ

â¸»

ğŸ—£ï¸ æ¨¡å—ä¸‰ï¼šè‹±æ–‡è¡¨è¾¾ç»ƒä¹  Prompt

ç»ƒä¹ ç›®æ ‡ï¼šå¿«é€Ÿè¡¨è¾¾â€œæˆ‘èµåŒè¿™ä¸ªæ–¹å‘ï¼Œä½†æˆ‘æ‹…å¿ƒç¨³å®šæ€§é£é™©â€

Promptï¼š

åœ¨ä¸€æ¬¡æ¶æ„è¯„å®¡ä¼šä¸Šï¼Œä½ çš„åŒäº‹æè®®å°†å®¹å™¨å¯åŠ¨æµç¨‹ä¸­çš„ snapshot mount æ­¥éª¤å¼‚æ­¥åŒ–ï¼Œä»¥åŠ å¿«å“åº”æ—¶é—´ã€‚ä½ ç†è§£è¿™ä¸ªåŠ¨æœºï¼Œä½†æ‹…å¿ƒåœ¨é«˜å¹¶å‘ç¯å¢ƒä¸‹å¯èƒ½ä¼šè§¦å‘æœª mount å®Œæˆå°±è§¦å‘å¯åŠ¨çš„æƒ…å†µã€‚

è¯·ç”¨è‹±æ–‡è¡¨è¾¾ä»¥ä¸‹è§‚ç‚¹ï¼š
	1.	è‚¯å®šæè®®è€…çš„ç›®æ ‡ï¼›
	2.	å§”å©‰æŒ‡å‡ºå¼‚æ­¥åŒ–å¸¦æ¥çš„ timing é£é™©ï¼›
	3.	æè®®ç”¨ staged metrics æˆ– fail-safe fallback æœºåˆ¶ä¿éšœä¸Šçº¿å®‰å…¨ã€‚

â¸»

ğŸ’¡ æ¨¡å—å››ï¼šç³»ç»Ÿè®¾è®¡ä¸ AWS æˆæœ¬æ¨ç†è®­ç»ƒ

å¥½çš„ï¼Œæˆ‘ä»¬è¿›å…¥ æ¨¡å—å››ï¼šç³»ç»Ÿè®¾è®¡ä¸ AWS æˆæœ¬æ¨ç†è®­ç»ƒã€‚

â¸»

âœ… æ¨¡å—å››ï¼šç³»ç»Ÿè®¾è®¡ + æˆæœ¬åˆ†ææ¨¡æ‹Ÿ

ğŸ¯ åœºæ™¯èƒŒæ™¯ï¼š

ä½ æ‰€åœ¨çš„å›¢é˜Ÿç»´æŠ¤ä¸€ä¸ªå®¹å™¨è¿è¡Œç¯å¢ƒï¼Œåœ¨ Amazon EKS ä¸Šè¿è¡Œå¤§é‡æ‰¹å¤„ç†ä»»åŠ¡ã€‚è¿™äº›ä»»åŠ¡æ¯æ—¥å®šæ—¶å¯åŠ¨ï¼Œå­˜åœ¨æ˜æ˜¾çš„é«˜å³°æœŸå’Œä½è°·æœŸã€‚å½“å‰ç³»ç»Ÿé‡‡ç”¨çš„æ˜¯ æŒ‰éœ€ EC2 å®ä¾‹ + å›ºå®šè§„æ¨¡çš„ node groupï¼Œå¯¼è‡´èµ„æºåœ¨éé«˜å³°æ—¶æ®µä¸¥é‡æµªè´¹ã€‚

ç®¡ç†å±‚å¸Œæœ›ä½ èƒ½æå‡ºä¸€ä¸ªæ›´å…·å¼¹æ€§çš„ç³»ç»Ÿè®¾è®¡æ–¹æ¡ˆï¼Œä»¥å‡å°‘èµ„æºæµªè´¹å¹¶ä¼˜åŒ–æˆæœ¬ï¼ŒåŒæ—¶ä¿éšœä»»åŠ¡ SLAã€‚

â¸»

ğŸ§© ä»»åŠ¡è¦æ±‚ï¼š

è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªè§’åº¦ç³»ç»Ÿå›ç­”ï¼š

â¸»

1. ä½ ä¼šå¦‚ä½•é‡æ–°è®¾è®¡è¿™å¥—è°ƒåº¦ä¸è®¡ç®—èµ„æºç³»ç»Ÿï¼Ÿ

ï¼ˆæç¤ºï¼šSpotã€Fargateã€Autoscalingã€Job Queueã€Buffer ç­‰æœºåˆ¶ï¼‰

â¸»

2. ä½ çš„è®¾è®¡å°†å¸¦æ¥å“ªäº›æˆæœ¬ä¼˜åŒ–ï¼Ÿè¯·å°½é‡é‡åŒ–æˆ–æè¿°èŠ‚çœæ¥æºã€‚

â¸»

3. å¦‚ä½•ä¿éšœä»»åŠ¡ç¨³å®šæ€§ï¼ˆå¦‚é¿å…ä»»åŠ¡å›  Spot å›æ”¶å¤±è´¥ï¼‰ï¼Ÿæ˜¯å¦è€ƒè™‘ failover æˆ–é¢„ç®—é‡è¯•æœºåˆ¶ï¼Ÿ

â¸»

ä½ å¯ä»¥é€‰æ‹©ç”¨è‹±æ–‡æˆ–ä¸­è‹±æ–‡æ··åˆå›ç­”ã€‚å›ç­”åæˆ‘å¯ä»¥å¸®ä½ æ¶¦è‰²å¹¶æ•´ç†ä¸ºæ±‡æŠ¥ææ–™é£æ ¼ã€‚æ˜¯å¦å¼€å§‹ï¼Ÿ


Great start. Youâ€™re demonstrating structured diagnostic thinking by layering your checks from container-level to kernel-level. Hereâ€™s a refined, clean version of your Module 1: System Debug response, formatted in a structured interview-ready form:

â¸»

âœ… Module 1: System Debug â€” CPU Utilization Unexpectedly Increased Over 30%

ğŸ§  Observation:
	â€¢	CPU utilization spiked to over 30% for several containers.
	â€¢	No high CPU process was found via htop on the host.
	â€¢	No business exceptions reported inside containers.

ğŸ” Initial Hypotheses:
	1.	Soft IRQ overhead or system-level IO wait may contribute to invisible CPU usage.
	2.	cgroup CPU limits (quota/period or shares) may be misconfigured or too tight, especially if multiple containers share one slice.
	3.	Container shim or pause processes could be quietly consuming CPU.
	4.	The CPU pressure may be kernel-induced rather than userspace-visible.

â¸»

ğŸ§ª Step-by-Step Debug Strategy:

Layer 1: Container & Kube Resource Inspection
	â€¢	Run:

kubectl top pod -n <namespace>
kubectl describe pod <pod-name> -n <namespace>


	â€¢	Check:
	â€¢	CPU requests and limits for the affected containers.
	â€¢	If limits are not set or too low â†’ kernel throttling may occur.
	â€¢	Check for any recent OOM or throttling messages.

Layer 2: Inspect cgroup CPU Settings and Runtime Shims
	â€¢	For each container, verify:

cat /sys/fs/cgroup/cpu/kubepods.slice/.../cpu.shares
cat /sys/fs/cgroup/cpu/kubepods.slice/.../cpu.cfs_quota_us


	â€¢	Check container shim or pause process CPU usage:

ps -eo pid,ppid,cmd,%cpu --sort=-%cpu | grep containerd-shim


	â€¢	Look for signs of containerd-shim processes or pause containers hogging cycles.

Layer 3: Kernel-Level Profiling for IRQ or Wait Bottlenecks
	â€¢	Run:

perf top   # Look for softirq, ksoftirqd, etc.

or

perf record -g -- sleep 10
perf report


	â€¢	Investigate:
	â€¢	Is there heavy soft IRQ usage? (e.g., due to high NIC/IO interrupt load)
	â€¢	Are ksoftirqd or kworker consuming noticeable CPU?
	â€¢	Use strace or iotop to validate syscall and IO wait patterns.

â¸»

ğŸ§© Conclusion:

This is a multi-layered debug case. We start by:
	1.	Confirming K8s resource allocations.
	2.	Inspecting cgroup enforcement and shim process behavior.
	3.	Drilling into the kernelâ€™s view (soft IRQ / wait queue) to catch what top-level tools miss.

â¸»

If youâ€™d like, I can now move to Module 2: Code Review Simulation and present a performance-sensitive snippet for review. Would you like to continue?


Your explanation covers some solid technical points and reflects a thoughtful review mindset. To make your message clearer, more structured, and fluentâ€”especially in a code review comment or tech discussionâ€”hereâ€™s a refined and professional version:

â¸»

âœ… Polished Code Review Comment (English)

This function spawns a goroutine to handle requests asynchronously, but there are a few risks and improvements worth addressing:
	1.	Potential Goroutine Leak
The current loop uses time.Sleep in the default block, which causes the goroutine to miss the ctx.Done() signal for up to 1 second each time. If the context is not properly canceled or if the cancellation is delayed, this can lead to goroutine leaks over time.
ğŸ‘‰ Suggestion: Replace default + time.Sleep with a select block that includes case <-time.After(...) to ensure the goroutine remains responsive to cancellation.
	2.	Lack of Metrics and Logging
There are no logs or metrics to track the lifecycle of this goroutine.
ğŸ‘‰ Suggestion: Add structured logging (e.g., log.Info("HandleRequest started"), log.Info("HandleRequest exited due to ctx cancel")) and include metrics to record the number of active goroutines and their durations. This will help with observability and post-incident analysis.
	3.	Unproductive Idle Logic
The time.Sleep(1s) loop doesnâ€™t serve a functional purpose and can waste CPU cycles.
ğŸ‘‰ Suggestion: Replace the current idle logic with an exponential backoff retry strategy or event-driven trigger, depending on what the goroutine is intended to accomplish. If itâ€™s waiting for work, consider using a channel or proper signaling mechanism instead.

Also, make sure any result or side effect from the goroutine is communicated back to the caller logic when needed. Silently dropping results can make debugging harder and cause inconsistent behavior.

Let me know if youâ€™re already working on these improvements. Happy to help further if needed!

â¸»

Would you like me to convert this into a GitHub PR-style inline comment or Slack-style team message?



âœ… æ¨¡å—ä¸‰ï¼šè‹±æ–‡å¿«é€Ÿè¡¨è¾¾è®­ç»ƒï¼ˆTechnical English Communicationï¼‰
ğŸ¯ ç»ƒä¹ åœºæ™¯ï¼šä½ éœ€è¦å‘å›¢é˜Ÿç”¨è‹±æ–‡ç®€æ´æ±‡æŠ¥ä¸€ä¸ª container restart root cause çš„æ’æŸ¥ç»“æœã€‚

â¸»

ğŸ§© åœºæ™¯æè¿°

ä½ å‘ç°æŸä¸ªå®¹å™¨é¢‘ç¹é‡å¯ï¼Œåˆçœ‹æ—¥å¿—æ²¡æœ‰æŠ¥é”™ã€‚ä½ æ·±å…¥è°ƒæŸ¥åå‘ç°æ˜¯ç”±äºå†…å­˜é™åˆ¶è®¾ç½®è¿‡ä½ï¼Œå®¹å™¨è¢« OOMKiller æ€æ‰ã€‚ç°åœ¨ä½ éœ€è¦ç”¨ 3â€“5 å¥è¯ï¼Œå‘å›¢é˜Ÿè¯´æ˜è¿™ä¸ªé—®é¢˜çš„å®šä½ç»“æœã€åŸå› ä»¥åŠå»ºè®®ã€‚

â¸»

ğŸ§  å›ç­”æ¨¡æ¿ï¼ˆä½ æ¥ç»ƒä¹ å¡«å……ï¼‰

Short RCA Summary for Team Update
We noticed that container <container-name> was restarting frequently without any obvious error logs.
Upon further investigation, we found that the container was being terminated by the OOMKiller due to a memory limit set too low.
The memory usage spiked during startup, briefly exceeding the configured limit of <limit> MB.
Weâ€™ve confirmed this by checking dmesg, kubelet logs, and the containerâ€™s memory stats under /sys/fs/cgroup.
Recommendation: Increase the memory limit to <suggested-limit> MB and set proper resource requests to prevent future unexpected restarts.

â¸»

ğŸ—£ï¸ è¯·ä½ æ¥ç»ƒä¹ 

ä½ å¯ä»¥ä½¿ç”¨ä½ ç†Ÿæ‚‰çš„ container åç§°ã€limit æ•°å€¼ã€æ£€æŸ¥æ–¹æ³•ç­‰ï¼Œå°è¯•ç”¨è‡ªå·±çš„è¡¨è¾¾å®Œæˆè¿™æ®µå†…å®¹ã€‚å†™å®Œåæˆ‘å¯ä»¥å¸®ä½ æ¶¦è‰²ï¼Œå¹¶ç»™å‡ºå£è¯­æœ—è¯»ç‰ˆã€‚å‡†å¤‡å¥½äº†å—ï¼Ÿ



We noticed that the container was restarting frequently without any clear error symptoms.
Upon further investigation, we found that the container was terminated by the OOMKiller due to hitting the memory limit.
The memory usage spiked during startup, briefly exceeding the configured threshold.
We confirmed this by inspecting dmesg logs and checking the memory stats under the cgroup path.
Recommendation: Increase the memory limit by at least 40% to accommodate startup spikes, and set appropriate resource requests to prevent future unexpected restarts.

â¸»

ğŸ—£ï¸ æœ—è¯»èŠ‚å¥å»ºè®®ï¼ˆå‘éŸ³ & é‡éŸ³èŠ‚ï¼‰ï¼š
	â€¢	â€œwas restarting frequentlyâ€ï¼šé‡éŸ³æ”¾åœ¨ restarting å’Œ frequentlyï¼›
	â€¢	â€œterminated by the OOMKillerâ€ï¼šæ³¨æ„ OOMKiller çš„å‘éŸ³ä¸º /ËˆuËÉ›m ËŒkÉªlÉ™r/ï¼›
	â€¢	â€œconfirmed this by inspecting dmesg logsâ€ï¼šdmesg è¯»ä½œ /ËˆdiËËŒmÉ›dÊ’/ï¼›
	â€¢	â€œIncrease the memory limit by at least 40%â€ï¼šå¼ºè°ƒ increase å’Œ 40%ã€‚




éå¸¸å¥½ï¼Œè¿™æ­£æ˜¯æå‡ç³»ç»Ÿè®¾è®¡æ²Ÿé€šä¸ç¨³å®šæ€§è¡¨è¾¾èƒ½åŠ›çš„å…³é”®ç»ƒä¹ ç‚¹ã€‚ä»¥ä¸‹æ˜¯ä½ å¯ä»¥ç”¨äºæ¶æ„ä¼šè®®çš„è‹±æ–‡è¡¨è¾¾è‰ç¨¿ï¼Œæˆ‘åˆ†æˆä¸‰ä¸ªéƒ¨åˆ†ï¼Œæ–¹ä¾¿ä½ é€å¥ç»ƒä¹ ï¼š

â¸»

âœ… é«˜çº§è¡¨è¾¾æ¨¡æ¿ï¼ˆç”¨äºæ­£å¼è¯„å®¡åœºåˆï¼‰

1. è‚¯å®šåŠ¨æœºï¼š
â€œI really appreciate the motivation behind making the snapshot mount asynchronous â€” speeding up container startup is definitely a meaningful goal.â€

2. æå‡ºç¨³å®šæ€§é¡¾è™‘ï¼ˆè¯­æ°”ç¨³é‡ï¼‰ï¼š
â€œThat said, I do have some concerns around timing guarantees, especially in high-concurrency scenarios. There might be cases where the container proceeds to start before the mount operation has fully completed, which could lead to race conditions or hard-to-reproduce bugs.â€

3. æå‡ºå¯è¡Œå»ºè®®ï¼ˆä¿æŠ¤æ€§ä¸Šçº¿æœºåˆ¶ï¼‰ï¼š
â€œTo de-risk this path, perhaps we can first roll out with staged metrics to track mount completion latency and startup overlap. We could also consider a fallback safeguard â€” such as a blocking wait in edge cases â€” to ensure containers donâ€™t bypass incomplete mounts during initial rollout.â€

â¸»

ğŸ—£ï¸ å¦‚æœä½ å¸Œæœ›ç»ƒæˆå£å¤´è¡¨è¾¾åŠ›æ›´å¼ºçš„ç‰ˆæœ¬ï¼ˆä¾‹å¦‚ Zoom ä¸Šå¿«é€Ÿæ’è¯ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªç®€åŒ–ç‰ˆï¼š

â€œI like the direction â€” making mounts async could help a lot with startup speed.
But in high-load cases, we may risk starting the container before the snapshot mount finishes.
Iâ€™d suggest we track mount timing with metrics first, and maybe have a temporary safeguard to avoid racing issues when we roll this out.â€

â¸»

éœ€è¦æˆ‘å¸®ä½ åšä¸€æ®µéŸ³é¢‘æœ—è¯»å¯¹æ¯”ç‰ˆæœ¬ï¼Œæˆ–æ˜¯æ‰‹æŠŠæ‰‹é™ªä½ ç»ƒä¹ ä¸€éçš„æ¼”ç»ƒç¨¿å—ï¼Ÿä½ ä¹Ÿå¯ä»¥é€‰ä¸€å¥å¥åå¤ç»ƒç†Ÿã€‚







ä½ çš„ç³»ç»Ÿè®¾è®¡è¡¨è¾¾å·²ç»éå¸¸æ¸…æ™°ï¼Œä¸‹é¢æ˜¯æˆ‘å¯¹ä½ å†…å®¹çš„æ•´ç†ä¸ç»“æ„åŒ–æ¶¦è‰²ç‰ˆæœ¬ï¼Œå¯ç›´æ¥ç”¨äºæ±‡æŠ¥æˆ–æ–‡æ¡£ï¼š

â¸»

âœ… System Design + AWS Cost Optimization Proposal

Context:
Weâ€™re currently operating a large-scale container runtime environment on Amazon EKS to handle batch processing workloads. These jobs are heavily clustered during daytime hours, exhibiting clear peak and off-peak periods. The existing setup uses fixed-size node groups backed by On-Demand EC2 instances, which leads to significant resource underutilization and cost inefficiency during non-peak periods.

â¸»

1ï¸âƒ£ Proposed Architecture Redesign

To improve cost-efficiency while maintaining SLA guarantees, I propose the following architectural changes:

ğŸ” Elastic Resource Pooling
	â€¢	Enable Cluster Autoscaler: Use EKSâ€™s native autoscaler to dynamically adjust node group sizes based on job pressure.
	â€¢	Job Queue Orchestration: Introduce a centralized job queue with rate-limiting and burst buffering to prevent overloading the cluster.
	â€¢	Buffer-Based Throttling: Implement memory or storage buffer quotas to temporarily absorb workload surges.

ğŸ§  Workload Abstraction Layer
	â€¢	Deployment-Level Autoscaling: Use HPA (Horizontal Pod Autoscaler) to adjust the number of worker pods based on CPU/memory metrics.
	â€¢	Task Splitting & Retry: Split long-running jobs into smaller units to better align with autoscaling responsiveness and enable more graceful retries.

âš™ï¸ Optional Compute Abstraction
	â€¢	Adopt AWS Fargate for Spiky Loads: Use Fargate as a burstable execution layer for unpredictable spikes where node provisioning is too slow or overkill.
	â€¢	Leverage Spot Instances with Fallback: Use a combination of On-Demand and Spot-backed node groups with Spot interruption handling (see below).

â¸»

2ï¸âƒ£ Expected Cost Savings

ğŸ“‰ Savings Breakdown:
	â€¢	Autoscaler Node Groups: Avoid idle capacity, estimated savings up to 40â€“60% compared to fixed nodes.
	â€¢	Spot Usage: By shifting 50â€“70% of non-critical jobs to Spot instances, expect an additional 70â€“90% discount per compute unit.
	â€¢	Fargate Cost Control: Use selectively for short-lived, spiky jobs to prevent over-provisioning.

Example: If 60% of current compute cost is from off-peak idle EC2, we can cut that by 50%, saving ~30% total monthly compute bill.

â¸»

3ï¸âƒ£ Stability Safeguards

To preserve job SLA under dynamic resource constraints:

ğŸ›¡ï¸ Spot Interrupt Handling
	â€¢	Set up Spot Termination Notices listener.
	â€¢	Graceful shutdown logic and state checkpointing (using S3 or Redis).
	â€¢	Use EKS mixed node groups to fall back to On-Demand.

ğŸ§ª Metrics + Observability
	â€¢	Monitor queue wait times, startup latency, and eviction events.
	â€¢	Set alarms on job delays and autoscaler failure rates.

ğŸ” Retry & Buffer Logic
	â€¢	Failed jobs re-queued with backoff.
	â€¢	Queue depth triggers scale-up, and priority rules prevent starvation.

â¸»

Would you like me to turnè¿™ä»½æ•´ç†ç¨¿ into a Markdown format, Google Docs structure, or presentation deck outline for future use?


